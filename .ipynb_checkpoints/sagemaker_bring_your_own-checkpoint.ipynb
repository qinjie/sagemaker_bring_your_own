{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Bring Your Own Algorithm Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Test-on-Local-Machine\" data-toc-modified-id=\"Test-on-Local-Machine-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Test on Local Machine</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Docker-Image\" data-toc-modified-id=\"Build-Docker-Image-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Build Docker Image</a></span></li><li><span><a href=\"#Local-Test\" data-toc-modified-id=\"Local-Test-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Local Test</a></span><ul class=\"toc-item\"><li><span><a href=\"#train_local.sh\" data-toc-modified-id=\"train_local.sh-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span><code>train_local.sh</code></a></span></li><li><span><a href=\"#serve_local.sh\" data-toc-modified-id=\"serve_local.sh-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span><code>serve_local.sh</code></a></span></li><li><span><a href=\"#predict.sh\" data-toc-modified-id=\"predict.sh-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span><code>predict.sh</code></a></span></li></ul></li><li><span><a href=\"#Publish-Image-to-ECR\" data-toc-modified-id=\"Publish-Image-to-ECR-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Publish Image to ECR</a></span><ul class=\"toc-item\"><li><span><a href=\"#Manual-Steps\" data-toc-modified-id=\"Manual-Steps-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Manual Steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Model-in-SageMaker\" data-toc-modified-id=\"Train-Model-in-SageMaker-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Train Model in SageMaker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-the-environment\" data-toc-modified-id=\"Set-up-the-environment-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Set up the environment</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#Create-an-estimator-and-fit-the-model\" data-toc-modified-id=\"Create-an-estimator-and-fit-the-model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Create an estimator and fit the model</a></span></li></ul></li><li><span><a href=\"#Host-Model-in-SageMaker\" data-toc-modified-id=\"Host-Model-in-SageMaker-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Host Model in SageMaker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choose-some-data-and-use-it-for-a-prediction\" data-toc-modified-id=\"Choose-some-data-and-use-it-for-a-prediction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Choose some data and use it for a prediction</a></span></li><li><span><a href=\"#Optional-cleanup\" data-toc-modified-id=\"Optional-cleanup-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Optional cleanup</a></span></li></ul></li><li><span><a href=\"#Run-Batch-Transform-Job\" data-toc-modified-id=\"Run-Batch-Transform-Job-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Run Batch Transform Job</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-Transform-Job\" data-toc-modified-id=\"Create-a-Transform-Job-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create a Transform Job</a></span></li><li><span><a href=\"#View-Output\" data-toc-modified-id=\"View-Output-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>View Output</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the image using Dockerfile in `container` folder.\n",
    "\n",
    "```sh\n",
    "cd container\n",
    "docker build -t sagemaker_bring_your_own . \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Test\n",
    "\n",
    "To test the algorithm and docker image, use the three shell scripts in the **`test`** folder. It builds the image and runs it in a container to train and test the model. It mounts a directory structure that mimics production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `train_local.sh`\n",
    "\n",
    "- Run the script with the name of the image. \n",
    "- It maps `test_dir` folder to `/opt/ml` folder. \n",
    "- Test data is placed in `test_dir/input/data`.\n",
    "- (Optional) Modify the file `test_dir/input/config/hyperparameters.json` to have the hyperparameter settings that you want to test (as strings).\n",
    "- Trained model will be saved to `test_dir/models` folder.\n",
    "\n",
    "```sh\n",
    "./train_local.sh sagemaker_bring_your_own\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `serve_local.sh`\n",
    "\n",
    "- Run this with the name of the image to serve the model after model is trained.\n",
    "\n",
    "```sh\n",
    "./serve_local.sh sagemaker_bring_your_own\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict.sh`\n",
    "\n",
    "- Run this with the name of a payload file and (optionally) the HTTP content type you want. The content type will default to `text/csv`. For example, you can run \n",
    "\n",
    "```sh\n",
    "./predict.sh payload.csv text/csv\n",
    "```\n",
    "\n",
    "- Alternatively, can run following command to test the prediction. Need to use full path in the curl command.\n",
    "\n",
    "```sh\n",
    "curl --data-binary @D:/tmp/sagemaker_bring_your_own/container/local_test/payload.csv -H \"Content-Type: text/csv\" -v http://localhost:8080/invocations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Image to ECR\n",
    "\n",
    "Run the `build_and_push.sh <IMAGE_NAME>` script in the folder `container`.\n",
    "\n",
    "```\n",
    "./build_and_push.sh sagemaker_bring_your_own\n",
    "```\n",
    "\n",
    "\n",
    "#### Manual Steps\n",
    "\n",
    "For debugging purpose, you can also run following commands one by one.\n",
    "\n",
    "1. With AWS CLI 2, login into AWS ECR.\n",
    "\n",
    "```sh\n",
    "aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 825935993978.dkr.ecr.ap-southeast-1.amazonaws.com\n",
    "```\n",
    "\n",
    "2. Tag local image with full ECR image name.\n",
    "\n",
    "```sh\n",
    "docker tag sagemaker_bring_your_own <ACCOUNT_ID>.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker_bring_your_own:latest\n",
    "```\n",
    "\n",
    "3. Push image to ECR.\n",
    "\n",
    "```sh\n",
    "docker push <ACCOUNT_ID>.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker_bring_your_own:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model in SageMaker\n",
    "\n",
    "After local test, use SageMaker to train models and use the model for hosting or batch transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "\n",
    "- Import libraries\n",
    "- Get SageMaker execution role\n",
    "- Get current AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup S3 data paths to input training data and output model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "IMAGE_NAME = 'sagemaker_bring_your_own'\n",
    "\n",
    "# Where the training data is located\n",
    "data_bucket = 'temp-305326993135'\n",
    "data_prefix = f'{IMAGE_NAME}/input/data'\n",
    "data_bucket_path = f's3://{data_bucket}'\n",
    "\n",
    "# Where to save code and model artifacts\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "# TEST\n",
    "output_bucket = 'temp-305326993135'\n",
    "output_prefix = f'sagemaker/{IMAGE_NAME}'\n",
    "output_bucket_path = f's3://{output_bucket}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copy training data from input path to designated folder in output path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file: sagemaker_bring_your_own/input/data/train/train.csv\n",
      "File not found: sagemaker_bring_your_own/input/data/test/test.csv\n",
      "File not found: sagemaker_bring_your_own/input/data/validation/validation.csv\n"
     ]
    }
   ],
   "source": [
    "from botocore.errorfactory import ClientError\n",
    "\n",
    "for data_category in ['train', 'test', 'validation']:\n",
    "    data_key = f'{data_prefix}/{data_category}/{data_category}.csv'\n",
    "    output_key = f'{output_prefix}/{data_category}/{data_category}.csv'\n",
    "    data_filename = f'{data_category}.csv'\n",
    "    try:\n",
    "        s3_client.download_file(data_bucket, data_key, data_filename)\n",
    "        s3_client.upload_file(data_filename, output_bucket, output_key)\n",
    "        print(f'Copied file: {data_key}')\n",
    "    except ClientError as ex:\n",
    "        print(f'File not found: {data_key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_name = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{IMAGE_NAME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job sagemaker-bring-your-own-2022-01-18-06-09-33\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "job_name = f\"{IMAGE_NAME.replace('_','-')}-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "print(\"Training job\", job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": full_image_name, \"TrainingInputMode\": \"File\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": f\"{output_bucket_path}/{output_prefix}/temp\"},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.c4.2xlarge\", \"VolumeSizeInGB\": 5},\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {},\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{output_bucket_path}/{output_prefix}/train\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateTrainingJob operation: Could not assume role arn:aws:iam::305326993135:role/u-role-prog-access-cloudadmin. Please ensure that the role exists and allows principal 'sagemaker.amazonaws.com' to assume the role.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m sage_client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker\u001b[39m\u001b[38;5;124m\"\u001b[39m, region_name\u001b[38;5;241m=\u001b[39mregion)\n\u001b[1;32m----> 2\u001b[0m \u001b[43msage_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcreate_training_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m status \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mdescribe_training_job(TrainingJobName\u001b[38;5;241m=\u001b[39mjob_name)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainingJobStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\botocore\\client.py:391\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m py_operation_name)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\botocore\\client.py:719\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    717\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Could not assume role arn:aws:iam::305326993135:role/u-role-prog-access-cloudadmin. Please ensure that the role exists and allows principal 'sagemaker.amazonaws.com' to assume the role."
     ]
    }
   ],
   "source": [
    "sage_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sage_client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n",
    "while status != \"Completed\" and status != \"Failed\":\n",
    "    print(status)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tree = sage.estimator.Estimator(\n",
    "    full_image_name,\n",
    "    role,\n",
    "    1,\n",
    "    \"ml.c4.2xlarge\",\n",
    "    output_path=f\"s3://{sess.default_bucket()}/output\"\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host Model in SageMaker\n",
    "You can use a trained model to get real time predictions using HTTP endpoint. Follow these steps to walk you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = tree.deploy(1, \"ml.m4.xlarge\", serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some data and use it for a prediction\n",
    "\n",
    "In order to do some predictions, we'll extract some of the data we used for training and do predictions against it. This is, of course, bad statistical practice, but a good way to see how the mechanism works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = pd.read_csv(\"data/iris.csv\", header=None)\n",
    "shape.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the label column in the training set\n",
    "shape.drop(shape.columns[[0]], axis=1, inplace=True)\n",
    "shape.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = [50 * i for i in range(3)]\n",
    "b = [40 + i for i in range(10)]\n",
    "indices = [i + j for i, j in itertools.product(a, b)]\n",
    "\n",
    "test_data = shape.iloc[indices[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is as easy as calling predict with the predictor we got back from deploy and the data we want to do predictions with. The serializers take care of doing the data conversions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.predict(test_data.values).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional cleanup\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch Transform Job\n",
    "You can use a trained model to get inference on large data sets by using [Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html). A batch transform job takes your input data S3 location and outputs the predictions to the specified S3 output folder. Similar to hosting, you can extract inferences for training data to test batch transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Transform Job\n",
    "We'll create an `Transformer` that defines how to use the container to get inference results on a data set. This includes the configuration we need to invoke SageMaker batch transform:\n",
    "\n",
    "* The __instance count__ which is the number of machines to use to extract inferences\n",
    "* The __instance type__ which is the type of machine to use to extract inferences\n",
    "* The __output path__ determines where the inference results will be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output_folder = \"batch-transform-output\"\n",
    "output_path = \"s3://{}/{}\".format(sess.default_bucket(), transform_output_folder)\n",
    "\n",
    "transformer = tree.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tranform() on the transfomer to get inference results against the data that we uploaded. You can use these options when invoking the transformer. \n",
    "\n",
    "* The __data_location__ which is the location of input data\n",
    "* The __content_type__ which is the content type set when making HTTP request to container to get prediction\n",
    "* The __split_type__ which is the delimiter used for splitting input data \n",
    "* The __input_filter__ which indicates the first column (ID) of the input will be dropped before making HTTP request to container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    data_location, content_type=\"text/csv\", split_type=\"Line\", input_filter=\"$[1:]\"\n",
    ")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the configuration options, see [CreateTransformJob API](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Output\n",
    "Lets read results of above transform job from s3 files and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = sess.boto_session.client(\"s3\")\n",
    "s3_client.download_file(\n",
    "    sess.default_bucket(), \"{}/iris.csv.out\".format(transform_output_folder), \"/tmp/iris.csv.out\"\n",
    ")\n",
    "with open(\"/tmp/iris.csv.out\") as f:\n",
    "    results = f.readlines()\n",
    "print(\"Transform results: \\n{}\".format(\"\".join(results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
