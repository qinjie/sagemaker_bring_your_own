{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Bring Your Own Algorithm Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Test-on-Local-Machine\" data-toc-modified-id=\"Test-on-Local-Machine-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Test on Local Machine</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Docker-Image\" data-toc-modified-id=\"Build-Docker-Image-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Build Docker Image</a></span></li><li><span><a href=\"#Local-Test\" data-toc-modified-id=\"Local-Test-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Local Test</a></span><ul class=\"toc-item\"><li><span><a href=\"#train_local.sh\" data-toc-modified-id=\"train_local.sh-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span><code>train_local.sh</code></a></span></li><li><span><a href=\"#serve_local.sh\" data-toc-modified-id=\"serve_local.sh-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span><code>serve_local.sh</code></a></span></li><li><span><a href=\"#predict.sh\" data-toc-modified-id=\"predict.sh-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span><code>predict.sh</code></a></span></li></ul></li><li><span><a href=\"#Publish-Image-to-ECR\" data-toc-modified-id=\"Publish-Image-to-ECR-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Publish Image to ECR</a></span><ul class=\"toc-item\"><li><span><a href=\"#Manual-Steps\" data-toc-modified-id=\"Manual-Steps-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Manual Steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Model-in-SageMaker\" data-toc-modified-id=\"Train-Model-in-SageMaker-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Train Model in SageMaker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-the-environment\" data-toc-modified-id=\"Set-up-the-environment-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Set up the environment</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#Create-an-estimator-and-fit-the-model\" data-toc-modified-id=\"Create-an-estimator-and-fit-the-model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Create an estimator and fit the model</a></span></li></ul></li><li><span><a href=\"#Host-Model-in-SageMaker\" data-toc-modified-id=\"Host-Model-in-SageMaker-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Host Model in SageMaker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choose-some-data-and-use-it-for-a-prediction\" data-toc-modified-id=\"Choose-some-data-and-use-it-for-a-prediction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Choose some data and use it for a prediction</a></span></li><li><span><a href=\"#Optional-cleanup\" data-toc-modified-id=\"Optional-cleanup-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Optional cleanup</a></span></li></ul></li><li><span><a href=\"#Run-Batch-Transform-Job\" data-toc-modified-id=\"Run-Batch-Transform-Job-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Run Batch Transform Job</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-Transform-Job\" data-toc-modified-id=\"Create-a-Transform-Job-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create a Transform Job</a></span></li><li><span><a href=\"#View-Output\" data-toc-modified-id=\"View-Output-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>View Output</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the image using Dockerfile in `container` folder.\n",
    "\n",
    "```sh\n",
    "cd container\n",
    "docker build -t sagemaker_bring_your_own . \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Test\n",
    "\n",
    "To test the algorithm and docker image, use the three shell scripts in the **`test`** folder. It builds the image and runs it in a container to train and test the model. It mounts a directory structure that mimics production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `train_local.sh`\n",
    "\n",
    "- Run the script with the name of the image. \n",
    "- It maps `test_dir` folder to `/opt/ml` folder. \n",
    "- Test data is placed in `test_dir/input/data`.\n",
    "- (Optional) Modify the file `test_dir/input/config/hyperparameters.json` to have the hyperparameter settings that you want to test (as strings).\n",
    "- Trained model will be saved to `test_dir/models` folder.\n",
    "\n",
    "```sh\n",
    "./train_local.sh sagemaker_bring_your_own\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `serve_local.sh`\n",
    "\n",
    "- Run this with the name of the image to serve the model after model is trained.\n",
    "\n",
    "```sh\n",
    "./serve_local.sh sagemaker_bring_your_own\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict.sh`\n",
    "\n",
    "- Run this with the name of a payload file and (optionally) the HTTP content type you want. The content type will default to `text/csv`. For example, you can run \n",
    "\n",
    "```sh\n",
    "./predict.sh payload.csv text/csv\n",
    "```\n",
    "\n",
    "- Alternatively, can run following command to test the prediction. Need to use full path in the curl command.\n",
    "\n",
    "```sh\n",
    "curl --data-binary @D:/tmp/sagemaker_bring_your_own/container/local_test/payload.csv -H \"Content-Type: text/csv\" -v http://localhost:8080/invocations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Image to ECR\n",
    "\n",
    "Run the `build_and_push.sh <IMAGE_NAME>` script in the folder `container`.\n",
    "\n",
    "```\n",
    "./build_and_push.sh sagemaker_bring_your_own\n",
    "```\n",
    "\n",
    "\n",
    "#### Manual Steps\n",
    "\n",
    "For debugging purpose, you can also run following commands one by one.\n",
    "\n",
    "1. With AWS CLI 2, login into AWS ECR.\n",
    "\n",
    "```sh\n",
    "aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin <ACCOUNT_ID>.dkr.ecr.ap-southeast-1.amazonaws.com\n",
    "```\n",
    "\n",
    "2. Tag local image with full ECR image name.\n",
    "\n",
    "```sh\n",
    "docker tag sagemaker_bring_your_own <ACCOUNT_ID>.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker_bring_your_own:latest\n",
    "```\n",
    "\n",
    "3. Push image to ECR.\n",
    "\n",
    "```sh\n",
    "docker push <ACCOUNT_ID>.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker_bring_your_own:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model in SageMaker\n",
    "\n",
    "After local test, use SageMaker to train models and use the model for hosting or batch transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the environment\n",
    "\n",
    "#### Copy Files to S3\n",
    "\n",
    "- Copy `test/test_dir/input` folder into the working S3 bucket, e.g. `s3://temp-305326993135/sagemaker_bring_your_own/`\n",
    "- Copy `test/data` folder into the working s3 bucket, e.g. ``s3://temp-305326993135/sagemaker_bring_your_own/`\n",
    "\n",
    "#### Setup SageMaker Notebook Instance\n",
    "\n",
    "- Create a SageMaker Notebook instance\n",
    "- Upload this Jupyter Notebook file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variables\n",
    "\n",
    "- Import libraries\n",
    "- Get SageMaker execution role\n",
    "- Get current AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup S3 data paths to input training data and output model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "IMAGE_NAME = 'sagemaker_bring_your_own'\n",
    "\n",
    "# Where the training data is located\n",
    "input_bucket = 'temp-305326993135'\n",
    "input_data_prefix = f'{IMAGE_NAME}/input/data'\n",
    "input_config_prefix = f'{IMAGE_NAME}/input/config'\n",
    "\n",
    "# Where to save code and model artifacts\n",
    "# output_bucket = sagemaker.Session().default_bucket()\n",
    "# TEST\n",
    "output_bucket = 'temp-305326993135'\n",
    "output_prefix = f'{IMAGE_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read hyper-parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "try:\n",
    "    hyper_param_file = f'{input_config_prefix}/hyperparameters.json'\n",
    "    response = s3_client.get_object(Bucket=input_bucket, Key=hyper_param_file)\n",
    "    content = response['Body']\n",
    "    hyperparameters = json.loads(content.read())\n",
    "except Exception as ex:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Estimator and Fit the Model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-18 07:53:46 Starting - Starting the training job...\n",
      "2022-01-18 07:54:09 Starting - Launching requested ML instancesProfilerReport-1642492425: InProgress\n",
      "...\n",
      "2022-01-18 07:54:44 Starting - Preparing the instances for training.........\n",
      "2022-01-18 07:56:10 Downloading - Downloading input data\n",
      "2022-01-18 07:56:10 Training - Downloading the training image..\u001b[34mRows in training data 150\u001b[0m\n",
      "\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "\n",
      "2022-01-18 07:56:46 Uploading - Uploading generated training model\n",
      "2022-01-18 07:57:10 Completed - Training job completed\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n"
     ]
    }
   ],
   "source": [
    "full_image_name = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{IMAGE_NAME}:latest\"\n",
    "\n",
    "model = sagemaker.estimator.Estimator(\n",
    "    base_job_name=IMAGE_NAME.replace('_', '-'),\n",
    "    image_uri=full_image_name,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.2xlarge\",\n",
    "    volume_size= 5,     # GB\n",
    "    output_path=f\"s3://{output_bucket}/{output_prefix}/output\",\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "# Set hyperparameters for the model training\n",
    "model.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Specify s3 folder which contains training data and its data type\n",
    "train_input = sagemaker.TrainingInput(s3_data=f's3://{input_bucket}/{input_data_prefix}/train/', \n",
    "                                      content_type='text/csv')\n",
    "\n",
    "model.fit({'train': train_input}, wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host Model in SageMaker\n",
    "You can use a trained model to get real time predictions using HTTP endpoint. Follow these steps to walk you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, \n",
    "                         instance_type=\"ml.m4.xlarge\", \n",
    "                         serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: sagemaker-bring-your-own-2022-01-18-08-36-25-531\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(f'Endpoint: {predictor.endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some data and use it for a prediction\n",
    "\n",
    "In order to do some predictions, we'll extract some of the data we used for training and do predictions against it. This is, of course, bad statistical practice, but a good way to see how the mechanism works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3\n",
       "0   5.0  3.5  1.3  0.3\n",
       "23  6.8  3.2  5.9  2.3\n",
       "21  6.9  3.1  5.1  2.3"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"payload.csv\", header=None)\n",
    "test_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is as easy as calling predict with the predictor we got back from deploy and the data we want to do predictions with. The serializers take care of doing the data conversions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(data=test_data.values).decode('utf-8')\n",
    "result = result.split()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Cleanup\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch Transform Job\n",
    "You can use a trained model to get inference on large data sets by using [Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html). A batch transform job takes your input data S3 location and outputs the predictions to the specified S3 output folder. Similar to hosting, you can extract inferences for training data to test batch transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Transform Job\n",
    "We'll create an `Transformer` that defines how to use the container to get inference results on a data set. This includes the configuration we need to invoke SageMaker batch transform:\n",
    "\n",
    "* The __instance count__ which is the number of machines to use to extract inferences\n",
    "* The __instance type__ which is the type of machine to use to extract inferences\n",
    "* The __output path__ determines where the inference results will be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_bucket = 'temp-305326993135'\n",
    "transform_prefix = f'{IMAGE_NAME}/data'\n",
    "transform_output_path = f's3://{transform_bucket}/{transform_prefix}'\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=transform_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tranform() on the transfomer to get inference results against the data that we uploaded. You can use these options when invoking the transformer. \n",
    "\n",
    "* The __data_location__ which is the location of input data\n",
    "* The __content_type__ which is the content type set when making HTTP request to container to get prediction\n",
    "* The __split_type__ which is the delimiter used for splitting input data \n",
    "* The __input_filter__ which indicates the first column (ID) of the input will be dropped before making HTTP request to container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n",
      "\u001b[34mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [10] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [10] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [15] [INFO] Booting worker with pid: 15\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34mLoad model from /opt/ml/model/model.pkl\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"GET /ping HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mInvoked with 150 records\u001b[0m\n",
      "\u001b[34mPerform prediction on 150 rows of input\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"POST /invocations HTTP/1.1\" 200 1400 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2022-01-18T08:57:38.291:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [10] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [10] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [15] [INFO] Booting worker with pid: 15\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2022-01-18 08:57:32 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[35mStarting the inference server with 4 workers.\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [10] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [10] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [14] [INFO] Booting worker with pid: 14\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [15] [INFO] Booting worker with pid: 15\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m[2022-01-18 08:57:32 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34mLoad model from /opt/ml/model/model.pkl\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"GET /ping HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mLoad model from /opt/ml/model/model.pkl\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"GET /ping HTTP/1.1\" 200 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mInvoked with 150 records\u001b[0m\n",
      "\u001b[34mPerform prediction on 150 rows of input\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"POST /invocations HTTP/1.1\" 200 1400 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mInvoked with 150 records\u001b[0m\n",
      "\u001b[35mPerform prediction on 150 rows of input\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [18/Jan/2022:08:57:38 +0000] \"POST /invocations HTTP/1.1\" 200 1400 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2022-01-18T08:57:38.291:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transform_input = f's3://{transform_bucket}/{transform_prefix}/iris.csv'\n",
    "transformer.transform(\n",
    "    data=transform_input, content_type=\"text/csv\", split_type=\"Line\", input_filter=\"$[1:]\"\n",
    ")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the configuration options, see [CreateTransformJob API](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Output\n",
    "\n",
    "The output file will be the input file name + `.out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'versicolor', 'virginica', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "s3_client = sagemaker.Session().boto_session.client(\"s3\")\n",
    "output_file =  f\"{transform_prefix}/iris.csv.out\"\n",
    "\n",
    "result = []\n",
    "response = s3_client.list_objects(Bucket=transform_bucket, Prefix=output_file)\n",
    "for o in response.get('Contents'):\n",
    "    data = s3_client.get_object(Bucket=transform_bucket, Key=o.get('Key'))\n",
    "    contents = data['Body'].read()\n",
    "    result.extend([line.strip() for line in contents.decode(\"utf-8\").split()])\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
